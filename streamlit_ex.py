import streamlit as st
from langchain_core.runnables import RunnablePassthrough, RunnableLambda
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_openai import ChatOpenAI
from langchain.memory import ConversationBufferMemory
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_text_splitters import CharacterTextSplitter
from langchain.retrievers import SelfQueryRetriever
import re
import time
from dotenv import load_dotenv

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

st.set_page_config(layout="wide")

@st.cache_resource
def preprocessing():
    print('íŒŒì¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘!')
    with open('datexts.txt', encoding='utf-8') as f:
        da = f.read()
    with open('detexts.txt', encoding='utf-8') as f:
        de = f.read()
    with open('aitexts.txt', encoding='utf-8') as f:
        ai = f.read()
    with open('cbtexts.txt', encoding='utf-8') as f:
        cb = f.read()
    with open('cetexts.txt', encoding='utf-8') as f:
        ce = f.read()
    with open('apitexts.txt', encoding='utf-8') as f:
        api = f.read()
    with open('metexts.txt', encoding='utf-8') as f:
        me = f.read()
    with open('dstexts.txt', encoding='utf-8') as f:
        ds = f.read()

    files = [da, de, ai, cb, ce, api, me, ds]

    print('êµ¬ë¶„ì ìƒì„±!')
    for i, file in enumerate(files):
        files[i] = file.replace('\n', '\n\n')

    # í…ìŠ¤íŠ¸ ë¶„í• 
    splitter = CharacterTextSplitter.from_tiktoken_encoder(
        chunk_size = 7500
    )

    print('ë¬¸ì¥ ìë¥´ê¸°!')
    for i, file in enumerate(files):
        files[i] = splitter.split_text(file)

    # OpenAI Embedding ëª¨ë¸ ì´ˆê¸°í™”
    print('ì„ë² ë”© ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°!')
    embeddings = OpenAIEmbeddings(model='text-embedding-3-large')

    # FAISS ë²¡í„° DB ìƒì„±
    metadatas = [[{'role': 'ë°ì´í„° ë¶„ì„ê°€'}], [{'role': 'ë°ì´í„° ì—”ì§€ë‹ˆì–´'}], [{'role': 'AI ê°œë°œì'}], [{'role': 'ì±—ë´‡ ê°œë°œì'}],
                [{'role': 'í´ë¼ìš°ë“œ ì—”ì§€ë‹ˆì–´'}], [{'role': 'API ê°œë°œì'}], [{'role': 'ë¨¸ì‹ ëŸ¬ë‹ ì—”ì§€ë‹ˆì–´'}], [{'role': 'ë°ì´í„° ì‚¬ì´ì–¸í‹°ìŠ¤íŠ¸'}]]

    alldb = FAISS.from_texts(texts=files[0], embedding=embeddings, metadatas=metadatas[0]*len(files[0]))

    print('DBì— ì ì¬ ì¤‘!')
    for i in range(1, len(files)):
            print('í˜„ì¬ ìœ„ì¹˜ëŠ”: ', i)
            db = FAISS.from_texts(texts=files[i], embedding=embeddings, metadatas=metadatas[i]*len(files[i]))
            alldb.merge_from(db)

    # ê²€ìƒ‰ê¸° ì„¤ì •
    print('ê²€ìƒ‰ê¸° ì„¤ì •!')
    retriever = alldb.as_retriever(
        search_kwargs={'k': 15}
    )

    return retriever

# ë©”ëª¨ë¦¬ ì´ˆê¸°í™” (ì§ì ‘ chat_history ì„¤ì • ì œê±°)
if 'memory' not in st.session_state:
    st.session_state.memory = ConversationBufferMemory(
        memory_key="chat_history", 
        return_messages=True
    )

retriever = preprocessing()

# ì‹œìŠ¤í…œ ë©”ì‹œì§€ í…œí”Œë¦¿ ì„¤ì •
system_message = """
    ë„ˆëŠ” ì‚¬ìš©ìê°€ ì…ë ¥í•œ ì§ˆë¬¸ì— ë”°ë¼ ì§ë¬´, ê²½ë ¥, ì„ í˜¸ ì§€ì—­ì— ë§ëŠ” ì±„ìš© ê³µê³ ë¥¼ ì¶œë ¥í•˜ëŠ” AIì•¼. ì•„ë˜ ì§€ì¹¨ì— ë”°ë¼ ì‘ì—…ì„ ìˆ˜í–‰í•´:

    1. questionì—ì„œ ì§ë¬´, ê²½ë ¥ ìˆ˜ì¤€(ì˜ˆ: ì‹ ì…, ê²½ë ¥), ì„ í˜¸ ì§€ì—­ì„ ì¶”ì¶œí•´.
    2. contextì— í¬í•¨ëœ ì±„ìš© ê³µê³ ë“¤ ì¤‘ questionì˜ ìš”êµ¬ì‚¬í•­ì— ë§ëŠ” ê³µê³ ë¥¼ ê²€ìƒ‰í•´.
    - questionì— í¬í•¨ëœ ì§ë¬´, ê²½ë ¥, ì§€ì—­ê³¼ contextì˜ ì§ë¬´, ê²½ë ¥, ì§€ì—­ì´ ë™ì¼í•œ ê²ƒë§Œ ì„ íƒí•´ì•¼ í•´.
    - ê²½ë ¥ ì—¬ë¶€ê°€ ê²½ë ¥ ë¬´ê´€ì´ê±°ë‚˜ ì‹ ì…ê³¼ ê²½ë ¥ì´ ëª¨ë‘ í¬í•¨ë˜ì—ˆë‹¤ë©´ ì‹ ì…ì¼ ë•Œë„ ê²½ë ¥ì¼ ë•Œë„ ëª¨ë‘ í¬í•¨í•´.
    3. ì¤‘ë³µ ê³µê³  ì¶œë ¥ ë°©ì§€
    - í•œ ë²ˆ ì¶œë ¥í•  ë•Œ ê°™ì€ ê³µê³ ë¥¼ ì—¬ëŸ¬ ë²ˆ ì¶œë ¥í•˜ì§€ë§ˆ.
    - contextì˜ ëª¨ë“  ê³µê³ ë“¤ ì¤‘ memoryì— ì´ë¯¸ ì €ì¥ë˜ì–´ ìˆëŠ” ê³µê³ ë¼ë©´ ì´ì „ì— ì´ë¯¸ ì¶œë ¥ë˜ì—ˆë˜ ì¤‘ë³µ ê³µê³ ì´ë¯€ë¡œ í•´ë‹¹ ê³µê³ ë¥¼ ì œì™¸í•´.
    4. ê°€ì ¸ì˜¤ëŠ” í–‰ì˜ ê°œìˆ˜ëŠ” 10ê°œ ì´í•˜ë¡œ ê°€ì ¸ì™€.
    5. ì¶œë ¥ í˜•ì‹ì€ Streamlitì˜ `st.markdown()` í•¨ìˆ˜ì™€ í˜¸í™˜ë˜ëŠ” Markdown í‘œ í˜•ì‹ì´ì–´ì•¼ í•˜ë©°, ë‹¤ìŒê³¼ ê°™ì€ í˜•ì‹ì„ ì‚¬ìš©í•´:
    - ì¤‘ë³µ ê³µê³ ì´ê±°ë‚˜ ê³µê³ ê°€ ë” ì´ìƒ ì—†ë‹¤ë©´ ê·¸ëƒ¥ ë‚¨ì€ ê³µê³ ë“¤ë§Œ ì¶œë ¥í•´.

    | idx | íšŒì‚¬ì´ë¦„ | ê³µê³ ì´ë¦„ | ì§€ì—­ | ì‚¬ì´íŠ¸ | URL |
    |-----|----------|----------|------|-------|-----|
    | 1   | íšŒì‚¬ì´ë¦„1 | ê³µê³ ì´ë¦„1 | ì§€ì—­1 | ì‚¬ì´íŠ¸1 | [URL](í•´ë‹¹ url) |
    | 2   | íšŒì‚¬ì´ë¦„2 | ê³µê³ ì´ë¦„2 | ì§€ì—­2 | ì‚¬ì´íŠ¸2 | [URL](í•´ë‹¹ url) |
    | ... | ...      | ...      | ...  | ... | ... |


    # memory: {chat_history}
    # context: {context}
    # question: {question}
    # answer:
"""

prompt = ChatPromptTemplate.from_messages([
    ('system', system_message),
    MessagesPlaceholder(variable_name='chat_history'),
    ('human', '{question}'),
])

coordination = """
    ë„ˆëŠ” ì§ë¬´ì— ëŒ€í•œ ì„¤ëª…ì„ ì¹œì ˆí•˜ê²Œ í•´ì£¼ëŠ” ì¡ ì½”ë””ì•¼.
    ì‚¬ìš©ìê°€ ì–´ë– í•œ ì§ë¬´ì— ëŒ€í•´ ì„¤ëª…í•´ë‹¬ë¼ê³  ì…ë ¥í•˜ë©´,
    í•´ë‹¹ ì§ë¬´ì˜ ì´ë¦„, ì£¼ìš” ì—…ë¬´ ë° ì±…ì„, í•„ìš”í•œ ì—­ëŸ‰(í•™ë ¥, ê²½í—˜, ê¸°ìˆ  ë“±)ì„ ì‚¬ìš©ìê°€ ì•Œì•„ë³´ê¸° ê¹”ë”í•˜ê²Œ ì„¤ëª…í•´ì¤˜!
    ë§Œì•½ ì‚¬ìš©ìê°€ íŠ¹ì • ë‚´ìš©ì— ëŒ€í•´ ì§ˆë¬¸ì„ í•œë‹¤ë©´ ë„¤ê°€ ì•Œê³  ìˆëŠ”ë§Œí¼ ì„¤ëª…í•´ì£¼ë©´ ë¼.
    ì ˆëŒ€ ì—†ëŠ” ë‚´ìš©ì„ ì°½ì¡°í•˜ë©´ ì•ˆë¼. ëª¨ë¥´ëŠ” ë‚´ìš©ì¼ ê²½ìš° 'ì•Œ ìˆ˜ ì—†ëŠ” ì •ë³´ì…ë‹ˆë‹¤.' ë¼ê³  ì¶œë ¥í•˜ë©´ ë¼.

    ë§Œì•½ ì‚¬ìš©ìê°€ ì§ë¬´ë¥¼ ì œì‹œí•˜ë©´ì„œ ì´ì „ì— ì¶œë ¥ëë˜ ê³µê³ ë¥¼ ëª¨ë‘ ì¶œë ¥í•´ë‹¬ë¼ê³  í•˜ë©´ memoryì— ì €ì¥ëœ í•´ë‹¹ ì§ë¬´ì˜ ê³µê³ ë“¤ì„ ì „ë¶€ ë°‘ì˜ í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•´ì¤˜.
    ë§Œì•½ ì‚¬ìš©ìê°€ ì´ì „ì— ë¬¼ì–´ë´¤ë˜ ëª¨ë“  ê³µê³ ë¥¼ ì¶œë ¥í•´ë‹¬ë¼ê³  í•œë‹¤ë©´ ì§ë¬´ ì´ë¦„ì„ ê¼­ í‘œì— ë„£ê³  ê°™ì´ ì¶œë ¥í•´ì¤˜ì•¼ ë¼.
    ì¶œë ¥ í˜•ì‹ì€ í‘œ í˜•ì‹ìœ¼ë¡œ ì¶œë ¥ë˜ë©°, ê° ê³µê³ ëŠ” ë²ˆí˜¸(idx), íšŒì‚¬ì´ë¦„, ê³µê³ ì´ë¦„, ì§ë¬´, URLì„ í¬í•¨í•´ì•¼ í•´.
    URLì€ Streamlitì—ì„œ ì§€ì› ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë§í¬ë¥¼ ìƒì„±í•´ì•¼ í•˜ë©°, ë‹¤ìŒê³¼ ê°™ì´ ì¶œë ¥í•˜ë©´ ë¼:

    | idx | íšŒì‚¬ì´ë¦„ | ê³µê³ ì´ë¦„ | ì§€ì—­ | ì§ë¬´ | URL |
    |-----|----------|----------|------|-----|-----|
    | 1   | íšŒì‚¬ì´ë¦„1 | ê³µê³ ì´ë¦„1 | ì§€ì—­1 | ì§ë¬´1 | [URL](í•´ë‹¹ url) |
    | 2   | íšŒì‚¬ì´ë¦„2 | ê³µê³ ì´ë¦„2 | ì§€ì—­2 | ì§ë¬´2 | [URL](í•´ë‹¹ url) |
    | ... | ...      | ...      | ...  | ... | ... |

    Streamlitì—ì„œ ì´ í‘œë¥¼ ì¶œë ¥í•  ë•ŒëŠ” `st.markdown()` í•¨ìˆ˜ì™€ Markdown í…Œì´ë¸” í˜•ì‹ì„ í™œìš©í•˜ë©´ ë¼.
    ì‚¬ìš©ìê°€ ì„ íƒí•  ìˆ˜ ìˆëŠ” ì§ë¬´ëŠ” ë°ì´í„° ë¶„ì„ê°€', 'ë°ì´í„° ì—”ì§€ë‹ˆì–´', 'AI ê°œë°œì', 'ì±—ë´‡ ê°œë°œì', 
    'í´ë¼ìš°ë“œ ì—”ì§€ë‹ˆì–´', 'API ê°œë°œì', 'ë¨¸ì‹ ëŸ¬ë‹ ì—”ì§€ë‹ˆì–´', 'ë°ì´í„° ì‚¬ì´ì–¸í‹°ìŠ¤íŠ¸' ê°€ ì¡´ì¬í•´.

    # memory: {chat_history}
    # question: {question}
    # answer: 
"""

prompt_qa = ChatPromptTemplate.from_messages([
    ('system', coordination),
    MessagesPlaceholder(variable_name='chat_history'),
    ('human', '{question}'),
])

# ê²€ìƒ‰ ë° ì¶œë ¥ í•¨ìˆ˜
def search_jobs_with_llm(question):
    # ê²€ìƒ‰ëœ ê²°ê³¼ë¥¼ LLMì—ê²Œ ì „ë‹¬í•˜ê¸° ìœ„í•œ context ìƒì„±
    results = retriever.get_relevant_documents(question)
    context = "\n".join([result.page_content for result in results])
    llm = ChatOpenAI(model='gpt-4o', temperature=0.1)
    # ë©”ëª¨ë¦¬ì—ì„œ ëŒ€í™” ê¸°ë¡ ë¡œë“œ
    memory_variables = st.session_state.memory.load_memory_variables({})
    chat_history = memory_variables.get('chat_history', [])

    # LLMì—ê²Œ ê²€ìƒ‰ëœ ê²°ê³¼ì™€ ì§ˆë¬¸ì„ ì „ë‹¬í•˜ì—¬ ê³µê³  ì¶œë ¥
    chain = (
        RunnablePassthrough.assign(context=RunnableLambda(lambda _: context))
        | RunnablePassthrough.assign(chat_history=RunnableLambda(lambda _: chat_history))
        | prompt
        | llm
    )
    
    # ì‘ë‹µ ìƒì„±
    response = chain.invoke({'question': question})
    
    # ì‘ë‹µì„ ë¬¸ìì—´ë¡œ ë³€í™˜ (content ì¶”ì¶œ)
    if hasattr(response, 'content'):
        response_text = response.content
    else:
        response_text = str(response)

    # ë©”ëª¨ë¦¬ì— ëŒ€í™” ê¸°ë¡ ì €ì¥
    st.session_state.memory.save_context(
        {'input': question},
        {'output': response_text},
    )

    return response_text

def process_user_question(question):
    llm = ChatOpenAI(model='gpt-4o', temperature=0.7)
    memory_variables = st.session_state.memory.load_memory_variables({})
    chat_history = memory_variables.get('chat_history', [])
    chain = (
        RunnablePassthrough.assign(chat_history=RunnableLambda(lambda _: chat_history))
        | prompt_qa
        | llm
    )

    response = chain.invoke({'question': question})

    # ì‘ë‹µì„ ë¬¸ìì—´ë¡œ ë³€í™˜ (content ì¶”ì¶œ)
    if hasattr(response, 'content'):
        response_text = response.content
    else:
        response_text = str(response)

    # ë©”ëª¨ë¦¬ì— ëŒ€í™” ê¸°ë¡ ì €ì¥
    st.session_state.memory.save_context(
        {'input': question},
        {'output': response_text},
    )

    return response_text

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state["messages"] = [{"role": "assistant", "content": "ì›í•˜ì‹œëŠ” ì§ë¬´, ì‹ ì…/ê²½ë ¥, ì§€ì—­ì„ ì„ íƒí•´ì£¼ì„¸ìš”\n\ní˜¹ì€ ì§ë¬´ì— ëŒ€í•œ ì§ˆë¬¸ì„ ì±„íŒ…ì— ì…ë ¥í•´ì£¼ì„¸ìš”"}]

# ì‚¬ì´ë“œë°”: ì§ë¬´, ê²½ë ¥, ì§€ì—­ ì„ íƒ
with st.sidebar:
    # ì§ë¬´ ì„ íƒ
    job_options = ['ë°ì´í„° ë¶„ì„ê°€', 'ë°ì´í„° ì—”ì§€ë‹ˆì–´', 'AI ê°œë°œì', 'ì±—ë´‡ ê°œë°œì', 
                   'í´ë¼ìš°ë“œ ì—”ì§€ë‹ˆì–´', 'API ê°œë°œì', 'ë¨¸ì‹ ëŸ¬ë‹ ì—”ì§€ë‹ˆì–´', 'ë°ì´í„° ì‚¬ì´ì–¸í‹°ìŠ¤íŠ¸']
    selected_job = st.selectbox("ì§ë¬´ë¥¼ ì„ íƒí•˜ì„¸ìš”:", job_options, key="selected_job")

    # ê²½ë ¥ ì„ íƒ
    exp_options = ['ì‹ ì…', 'ê²½ë ¥']
    selected_exp = st.selectbox("ì‹ ì… ë˜ëŠ” ê²½ë ¥ì„ ì„ íƒí•˜ì„¸ìš”:", exp_options, key="selected_exp")

    # ì§€ì—­ ì„ íƒ
    loc_options = ['ì„œìš¸', 'ê²½ê¸°', 'ì¸ì²œ', 'ëŒ€ì „', 'ê´‘ì£¼', 'ëŒ€êµ¬', 'ìš¸ì‚°', 'ë¶€ì‚°', 'ê°•ì›', 
                   'ì„¸ì¢…', 'ì¶©ë¶', 'ì¶©ë‚¨', 'ì „ë¶', 'ì „ë‚¨', 'ê²½ë¶', 'ê²½ë‚¨', 'ì œì£¼', 'í•´ì™¸']
    selected_loc = st.multiselect("ì§€ì—­ì„ ì„ íƒí•˜ì„¸ìš”:", loc_options, key="selected_loc")

    # ì±„ìš© ê³µê³  ê²€ìƒ‰ ë²„íŠ¼
    if st.button("ğŸ” ì¡°íšŒ"):
        if selected_loc:
            loc_text = ", ".join(selected_loc)
        else:
            loc_text = "ëª¨ë“  ì§€ì—­"
        query = f"{loc_text}ì—ì„œ {selected_exp}ì¸ {selected_job} ì§ë¬´ë¥¼ ì±„ìš©í•˜ëŠ” ê³µê³  ì•Œë ¤ì¤˜"
        st.session_state["messages"].append({
            "role": "user",
            "content": query
        })
        
        # LLMì— ê²€ìƒ‰ ìš”ì²­
        search_start = time.time()
        response = search_jobs_with_llm(query)
        search_end = time.time()
        print(f'íƒìƒ‰ ì™„ë£Œ! {search_end-search_start:.2f}ì´ˆ')
        st.session_state["messages"].append({
            "role": "assistant",
            "content": response
        })

# ì•± ì œëª© ë° ì„¤ëª…
st.title("Job Search Chatbot ğŸ’­")
st.caption("ğŸš€ A Streamlit chatbot powered by OpenAI")
# st.write(st.session_state.memory.load_memory_variables({}))
# ê¸°ì¡´ ì±„íŒ… ë©”ì‹œì§€ ì¶œë ¥
for msg in st.session_state.messages:
    st.chat_message(msg["role"]).write(msg["content"])

# ì‚¬ìš©ì ì§ˆë¬¸ ì…ë ¥ ì²˜ë¦¬
user_input = st.chat_input("ì§ë¬´ì— ëŒ€í•œ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:")
if user_input:
    st.session_state["messages"].append({"role": "user", "content": user_input})
    st.chat_message("user").write(user_input)
    response = process_user_question(user_input)
    st.session_state["messages"].append({
        "role": "assistant",
        "content": response
    })
    st.chat_message("assistant").write(response)