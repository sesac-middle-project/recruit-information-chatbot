import streamlit as st
from langchain_core.runnables import RunnablePassthrough, RunnableLambda
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_openai import ChatOpenAI
from langchain.memory import ConversationBufferMemory
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_text_splitters import CharacterTextSplitter
import re
from dotenv import load_dotenv

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

@st.cache_resource
def preprocessing():
    # ì±„ìš© ë°ì´í„° íŒŒì¼ ì½ê¸°
    with open('jobs.txt', encoding='utf-8') as f:
        file = f.read()

    # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
    chfile = file.replace('\n', '\n\n')

    # í…ìŠ¤íŠ¸ ë¶„í• 
    splitter = CharacterTextSplitter.from_tiktoken_encoder(chunk_size=400)
    lines = splitter.split_text(chfile)

    # OpenAI Embedding ëª¨ë¸ ì´ˆê¸°í™”
    embeddings = OpenAIEmbeddings(model='text-embedding-3-small')

    # FAISS ë²¡í„° DB ìƒì„±
    db = FAISS.from_texts(texts=lines, embedding=embeddings)

    # ê²€ìƒ‰ê¸° ì„¤ì •
    retriever = db.as_retriever(search_kwargs={'k': 100})

    return retriever


# ë©”ëª¨ë¦¬ ì´ˆê¸°í™” (ì§ì ‘ chat_history ì„¤ì • ì œê±°)
if 'memory' not in st.session_state:
    st.session_state.memory = ConversationBufferMemory(
        memory_key="chat_history", 
        return_messages=True
    )

# LLM ì„¤ì •


retriever = preprocessing()

# ì‹œìŠ¤í…œ ë©”ì‹œì§€ í…œí”Œë¦¿ ì„¤ì •
system_message = """
    ë„ˆëŠ” ì…ë ¥ëœ question ì†ì— í¬í•¨ëœ ì§ë¬´, ê²½ë ¥, ì„ í˜¸ ì§€ì—­ì— ë§ëŠ” ì±„ìš© ê³µê³ ë¥¼ ë‹¤ì„¯ ê°œì”© ì¶œë ¥í•˜ëŠ” íƒìƒ‰ AIì•¼.
    ê²€ìƒ‰ëœ contextë“¤ ì¤‘ ì…ë ¥ëœ ì§ë¬´ì™€ ê°™ì€ ì¹´í…Œê³ ë¦¬ì¸ ì±„ìš© ê³µê³ ë¥¼ ì°¾ì•„ì˜¤ë©´ ë¼.
    ë°ì´í„° ì¤‘ ê²½ë ¥ ì—¬ë¶€ê°€ ì‹ ì…, ê²½ë ¥ì¸ ê²½ìš° ì‹ ì…ì¼ ë•Œì™€ ê²½ë ¥ì¼ ë•Œ ë‘˜ ë‹¤ ì¶”ê°€í•´ë„ ë¼.
    ê³µê³ ì´ë¦„ì— ì§ë¬´ê°€ ë“¤ì–´ê°€ëŠ” ê³µê³ ë¥¼ ìš°ì„ ìœ¼ë¡œ ì¶œë ¥í•˜ê³ , ê·¸ ì´ì™¸ì—ëŠ” ëœë¤ìœ¼ë¡œ ì¶œë ¥í•´ì¤˜.
    ê³µê³ ë¥¼ ê°€ì ¸ì˜¬ ë•ŒëŠ” ì´ì „ì— ë„¤ê°€ ê°€ì ¸ì˜¨ ê³µê³ ë“¤ì„ ì œì™¸í•˜ê³  ë‚˜ë¨¸ì§€ ê³µê³ ë“¤ ì¤‘ ë‹¤ì„¯ ê°œë¥¼ ë½‘ì•„ì„œ ê°€ì ¸ì™€ì„œ ì¶œë ¥í•´ì¤˜.
    ì¶œë ¥ í˜•ì‹ì€ í‘œ í˜•ì‹ìœ¼ë¡œ ì¶œë ¥ë˜ë©°, ê° ê³µê³ ëŠ” ë²ˆí˜¸(idx), íšŒì‚¬ì´ë¦„, ê³µê³ ì´ë¦„, ì§€ì—­, URLì„ í¬í•¨í•´ì•¼ í•´.
    URLì€ Streamlitì—ì„œ ì§€ì› ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë§í¬ë¥¼ ìƒì„±í•´ì•¼ í•˜ë©°, ë‹¤ìŒê³¼ ê°™ì´ ì¶œë ¥í•˜ë©´ ë¼:

    | idx | íšŒì‚¬ì´ë¦„ | ê³µê³ ì´ë¦„ | ì§€ì—­ | URL |
    |-----|----------|----------|------|-----|
    | 1   | íšŒì‚¬ì´ë¦„1 | ê³µê³ ì´ë¦„1 | ì§€ì—­1 | [URL](í•´ë‹¹ url) |
    | 2   | íšŒì‚¬ì´ë¦„2 | ê³µê³ ì´ë¦„2 | ì§€ì—­2 | [URL](í•´ë‹¹ url) |
    | ... | ...      | ...      | ...  | ... |

    Streamlitì—ì„œ ì´ í‘œë¥¼ ì¶œë ¥í•  ë•ŒëŠ” `st.markdown()` í•¨ìˆ˜ì™€ Markdown í…Œì´ë¸” í˜•ì‹ì„ í™œìš©í•˜ë©´ ë¼.

    # context: {context}
    # question: {question}
    # answer:
"""

prompt = ChatPromptTemplate.from_messages([
    ('system', system_message),
    MessagesPlaceholder(variable_name='chat_history'),
    ('human', '{question}'),
])

coordination = """
    ë„ˆëŠ” ì§ë¬´ì— ëŒ€í•œ ì„¤ëª…ì„ ì¹œì ˆí•˜ê²Œ í•´ì£¼ëŠ” ì¡ ì½”ë””ì•¼.
    ì‚¬ìš©ìê°€ ì–´ë– í•œ ì§ë¬´ì— ëŒ€í•´ ì„¤ëª…í•´ë‹¬ë¼ê³  ì…ë ¥í•˜ë©´,
    í•´ë‹¹ ì§ë¬´ì˜ ì´ë¦„, ì£¼ìš” ì—…ë¬´ ë° ì±…ì„, í•„ìš”í•œ ì—­ëŸ‰(í•™ë ¥, ê²½í—˜, ê¸°ìˆ  ë“±)ì„ ì‚¬ìš©ìê°€ ì•Œì•„ë³´ê¸° ê¹”ë”í•˜ê²Œ ì„¤ëª…í•´ì¤˜!
    ë§Œì•½ ì‚¬ìš©ìê°€ íŠ¹ì • ë‚´ìš©ì— ëŒ€í•´ ì§ˆë¬¸ì„ í•œë‹¤ë©´ ë„¤ê°€ ì•Œê³  ìˆëŠ”ë§Œí¼ ì„¤ëª…í•´ì£¼ë©´ ë¼.
    ì ˆëŒ€ ì—†ëŠ” ë‚´ìš©ì„ ì°½ì¡°í•˜ë©´ ì•ˆë¼. ëª¨ë¥´ëŠ” ë‚´ìš©ì¼ ê²½ìš° 'ì•Œ ìˆ˜ ì—†ëŠ” ì •ë³´ì…ë‹ˆë‹¤.' ë¼ê³  ì¶œë ¥í•˜ë©´ ë¼.

    ë§Œì•½ ì‚¬ìš©ìê°€ ì§ë¬´ë¥¼ ì œì‹œí•˜ë©´ì„œ ì´ì „ì— ì¶œë ¥ëë˜ ê³µê³ ë¥¼ ëª¨ë‘ ì¶œë ¥í•´ë‹¬ë¼ê³  í•˜ë©´ memoryì— ì €ì¥ëœ í•´ë‹¹ ì§ë¬´ì˜ ê³µê³ ë“¤ì„ ì „ë¶€ ë°‘ì˜ í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•´ì¤˜.
    ë§Œì•½ ì‚¬ìš©ìê°€ ì´ì „ì— ë¬¼ì–´ë´¤ë˜ ëª¨ë“  ê³µê³ ë¥¼ ì¶œë ¥í•´ë‹¬ë¼ê³  í•œë‹¤ë©´ ì§ë¬´ ì´ë¦„ì„ ê¼­ í‘œì— ë„£ê³  ê°™ì´ ì¶œë ¥í•´ì¤˜ì•¼ ë¼.
    ì¶œë ¥ í˜•ì‹ì€ í‘œ í˜•ì‹ìœ¼ë¡œ ì¶œë ¥ë˜ë©°, ê° ê³µê³ ëŠ” ë²ˆí˜¸(idx), íšŒì‚¬ì´ë¦„, ê³µê³ ì´ë¦„, ì§ë¬´, URLì„ í¬í•¨í•´ì•¼ í•´.
    URLì€ Streamlitì—ì„œ ì§€ì› ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë§í¬ë¥¼ ìƒì„±í•´ì•¼ í•˜ë©°, ë‹¤ìŒê³¼ ê°™ì´ ì¶œë ¥í•˜ë©´ ë¼:

    | idx | íšŒì‚¬ì´ë¦„ | ê³µê³ ì´ë¦„ | ì§€ì—­ | ì§ë¬´ | URL |
    |-----|----------|----------|------|-----|-----|
    | 1   | íšŒì‚¬ì´ë¦„1 | ê³µê³ ì´ë¦„1 | ì§€ì—­1 | ì§ë¬´1 | [URL](í•´ë‹¹ url) |
    | 2   | íšŒì‚¬ì´ë¦„2 | ê³µê³ ì´ë¦„2 | ì§€ì—­2 | ì§ë¬´2 | [URL](í•´ë‹¹ url) |
    | ... | ...      | ...      | ...  | ... | ... |

    Streamlitì—ì„œ ì´ í‘œë¥¼ ì¶œë ¥í•  ë•ŒëŠ” `st.markdown()` í•¨ìˆ˜ì™€ Markdown í…Œì´ë¸” í˜•ì‹ì„ í™œìš©í•˜ë©´ ë¼.

    # memory: {chat_history}
    # question: {question}
    # answer: 
"""

prompt_qa = ChatPromptTemplate.from_messages([
    ('system', coordination),
    MessagesPlaceholder(variable_name='chat_history'),
    ('human', '{question}'),
])

# ê²€ìƒ‰ ë° ì¶œë ¥ í•¨ìˆ˜
def search_jobs_with_llm(question):
    # ê²€ìƒ‰ëœ ê²°ê³¼ë¥¼ LLMì—ê²Œ ì „ë‹¬í•˜ê¸° ìœ„í•œ context ìƒì„±
    results = retriever.get_relevant_documents(question)
    context = "\n".join([result.page_content for result in results])
    llm = ChatOpenAI(model='gpt-4o', temperature=0.1)
    # ë©”ëª¨ë¦¬ì—ì„œ ëŒ€í™” ê¸°ë¡ ë¡œë“œ
    memory_variables = st.session_state.memory.load_memory_variables({})
    chat_history = memory_variables.get('chat_history', [])

    # LLMì—ê²Œ ê²€ìƒ‰ëœ ê²°ê³¼ì™€ ì§ˆë¬¸ì„ ì „ë‹¬í•˜ì—¬ ê³µê³  ì¶œë ¥
    chain = (
        RunnablePassthrough.assign(context=RunnableLambda(lambda _: context))
        | RunnablePassthrough.assign(chat_history=RunnableLambda(lambda _: chat_history))
        | prompt
        | llm
    )
    
    # ì‘ë‹µ ìƒì„±
    response = chain.invoke({'question': question})
    
    # ì‘ë‹µì„ ë¬¸ìì—´ë¡œ ë³€í™˜ (content ì¶”ì¶œ)
    if hasattr(response, 'content'):
        response_text = response.content
    else:
        response_text = str(response)

    # ë©”ëª¨ë¦¬ì— ëŒ€í™” ê¸°ë¡ ì €ì¥
    st.session_state.memory.save_context(
        {'input': question},
        {'output': response_text},
    )

    return response_text

def process_user_question(question):
    llm = ChatOpenAI(model='gpt-4o', temperature=0.7)
    memory_variables = st.session_state.memory.load_memory_variables({})
    chat_history = memory_variables.get('chat_history', [])
    chain = (
        RunnablePassthrough.assign(chat_history=RunnableLambda(lambda _: chat_history))
        | prompt_qa
        | llm
    )

    response = chain.invoke({'question': question})

    # ì‘ë‹µì„ ë¬¸ìì—´ë¡œ ë³€í™˜ (content ì¶”ì¶œ)
    if hasattr(response, 'content'):
        response_text = response.content
    else:
        response_text = str(response)

    # ë©”ëª¨ë¦¬ì— ëŒ€í™” ê¸°ë¡ ì €ì¥
    st.session_state.memory.save_context(
        {'input': question},
        {'output': response_text},
    )

    return response_text

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state["messages"] = [{"role": "assistant", "content": "ì›í•˜ì‹œëŠ” ì§ë¬´, ì‹ ì…/ê²½ë ¥, ì§€ì—­ì„ ì„ íƒí•´ì£¼ì„¸ìš”\n\ní˜¹ì€ ì§ë¬´ì— ëŒ€í•œ ì§ˆë¬¸ì„ ì±„íŒ…ì— ì…ë ¥í•´ì£¼ì„¸ìš”"}]

# ì‚¬ì´ë“œë°”: ì§ë¬´, ê²½ë ¥, ì§€ì—­ ì„ íƒ
with st.sidebar:
    # ì§ë¬´ ì„ íƒ
    job_options = ['ë°ì´í„° ë¶„ì„ê°€', 'ë°ì´í„° ì—”ì§€ë‹ˆì–´', 'AI ê°œë°œì', 'ì±—ë´‡ ê°œë°œì', 
                   'í´ë¼ìš°ë“œ ì—”ì§€ë‹ˆì–´', 'API ê°œë°œì', 'ë¨¸ì‹ ëŸ¬ë‹ ì—”ì§€ë‹ˆì–´', 'ë°ì´í„° ì‚¬ì´ì–¸í‹°ìŠ¤íŠ¸']
    selected_job = st.selectbox("ì§ë¬´ë¥¼ ì„ íƒí•˜ì„¸ìš”:", job_options, key="selected_job")

    # ê²½ë ¥ ì„ íƒ
    exp_options = ['ì‹ ì…', 'ê²½ë ¥']
    selected_exp = st.selectbox("ì‹ ì… ë˜ëŠ” ê²½ë ¥ì„ ì„ íƒí•˜ì„¸ìš”:", exp_options, key="selected_exp")

    # ì§€ì—­ ì„ íƒ
    loc_options = ['ì„œìš¸', 'ê²½ê¸°', 'ì¸ì²œ', 'ëŒ€ì „', 'ê´‘ì£¼', 'ëŒ€êµ¬', 'ìš¸ì‚°', 'ë¶€ì‚°', 'ê°•ì›', 
                   'ì„¸ì¢…', 'ì¶©ë¶', 'ì¶©ë‚¨', 'ì „ë¶', 'ì „ë‚¨', 'ê²½ë¶', 'ê²½ë‚¨', 'ì œì£¼', 'í•´ì™¸']
    selected_loc = st.multiselect("ì§€ì—­ì„ ì„ íƒí•˜ì„¸ìš”:", loc_options, key="selected_loc")

    # ì±„ìš© ê³µê³  ê²€ìƒ‰ ë²„íŠ¼
    if st.button("ğŸ” ì¡°íšŒ"):
        if selected_loc:
            loc_text = ", ".join(selected_loc)
        else:
            loc_text = "ëª¨ë“  ì§€ì—­"
        query = f"{loc_text}ì—ì„œ {selected_exp}ì¸ {selected_job} ì§ë¬´ë¥¼ ì±„ìš©í•˜ëŠ” ê³µê³  ì•Œë ¤ì¤˜"
        st.session_state["messages"].append({
            "role": "user",
            "content": query
        })
        
        # LLMì— ê²€ìƒ‰ ìš”ì²­
        response = search_jobs_with_llm(query)
        st.session_state["messages"].append({
            "role": "assistant",
            "content": response
        })

# ì•± ì œëª© ë° ì„¤ëª…
st.title("Job Search Chatbot ğŸ’­")
st.caption("ğŸš€ A Streamlit chatbot powered by OpenAI")

# ê¸°ì¡´ ì±„íŒ… ë©”ì‹œì§€ ì¶œë ¥
for msg in st.session_state.messages:
    st.chat_message(msg["role"]).write(msg["content"])

# ì‚¬ìš©ì ì§ˆë¬¸ ì…ë ¥ ì²˜ë¦¬
user_input = st.chat_input("ì§ë¬´ì— ëŒ€í•œ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:")
if user_input:
    st.session_state["messages"].append({"role": "user", "content": user_input})
    st.chat_message("user").write(user_input)
    response = process_user_question(user_input)
    st.session_state["messages"].append({
        "role": "assistant",
        "content": response
    })
    st.chat_message("assistant").write(response)