import os
import re
import streamlit as st
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings
from langchain_text_splitters import CharacterTextSplitter
from langchain.prompts import PromptTemplate
from langchain.memory import ConversationBufferMemory
from langchain_core.runnables import RunnablePassthrough, RunnableLambda
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_openai import ChatOpenAI
from dotenv import load_dotenv
load_dotenv()

# OpenAI API í‚¤ í™•ì¸
openai_api_key = os.getenv("OPENAI_API_KEY")
if not openai_api_key:
    st.info("OpenAI API Keyê°€ í•„ìš”í•©ë‹ˆë‹¤. í™˜ê²½ ë³€ìˆ˜ë¥¼ í†µí•´ ì„¤ì •í•˜ì„¸ìš”.")
    st.stop()

# ìºì‹œëœ ë¦¬ì†ŒìŠ¤ë¥¼ ìœ„í•œ í•¨ìˆ˜
@st.cache_resource
def preprocessing():
    # ì±„ìš© ë°ì´í„° íŒŒì¼ ì½ê¸°
    with open('jobs.txt', encoding='utf-8') as f:
        file = f.read()

    # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
    chfile = file.replace('\n', '\n\n')

    # í…ìŠ¤íŠ¸ ë¶„í• 
    splitter = CharacterTextSplitter.from_tiktoken_encoder(chunk_size=400)
    lines = splitter.split_text(chfile)

    # OpenAI Embedding ëª¨ë¸ ì´ˆê¸°í™”
    embeddings = OpenAIEmbeddings(model='text-embedding-3-small')

    # FAISS ë²¡í„° DB ìƒì„±
    db = FAISS.from_texts(texts=lines, embedding=embeddings)

    # ê²€ìƒ‰ê¸° ì„¤ì •
    retriever = db.as_retriever(search_kwargs={'k': 100})

    return retriever

# ë©”ëª¨ë¦¬ ì´ˆê¸°í™” (ì§ì ‘ chat_history ì„¤ì • ì œê±°)
if 'memory' not in st.session_state:
    st.session_state.memory = ConversationBufferMemory(
        memory_key="chat_history", 
        return_messages=True
    )
st.write(st.session_state.memory.load_memory_variables({}))

# LLM ì„¤ì •
llm = ChatOpenAI(model='gpt-4o', temperature=0.1)

retriever = preprocessing()

# ì‹œìŠ¤í…œ ë©”ì‹œì§€ í…œí”Œë¦¿ ì„¤ì •
system_message = """
    ë„ˆëŠ” ì…ë ¥ëœ question ì†ì— í¬í•¨ëœ ì§ë¬´, ê²½ë ¥, ì„ í˜¸ ì§€ì—­ì— ë§ëŠ” ì±„ìš© ê³µê³ ë¥¼ ë‹¤ì„¯ ê°œì”© ì¶œë ¥í•˜ëŠ” íƒìƒ‰ AIì•¼.
    ê²€ìƒ‰ëœ contextë“¤ ì¤‘ ì…ë ¥ëœ ì§ë¬´ì™€ ê°™ì€ ì¹´í…Œê³ ë¦¬ì¸ ì±„ìš© ê³µê³ ë¥¼ ì°¾ì•„ì˜¤ë©´ ë¼.
    ê³µê³ ì´ë¦„ì— ì§ë¬´ê°€ ë“¤ì–´ê°€ëŠ” ê³µê³ ë¥¼ ìš°ì„ ìœ¼ë¡œ ì¶œë ¥í•˜ê³ , ê·¸ ì´ì™¸ì—ëŠ” ëœë¤ìœ¼ë¡œ ì¶œë ¥í•´ì¤˜.
    ë§Œì•½ ì‚¬ìš©ìê°€ ë” ë§ì€ ê³µê³ ë¥¼ í•„ìš”ë¡œ í•œë‹¤ë©´, ê°™ì€ ì¹´í…Œê³ ë¦¬ì—ì„œ ì´ì „ì— ë„¤ê°€ ê°€ì ¸ì˜¨ ê³µê³ ë“¤ì„ ì œì™¸í•˜ê³  ë‚˜ë¨¸ì§€ ê³µê³ ë“¤ ì¤‘ ë‹¤ì„¯ ê°œë¥¼ ë½‘ì•„ì„œ ê°€ì ¸ì™€ì„œ ì¶œë ¥í•´ì¤˜.
    ì¶œë ¥ í˜•ì‹ì€ í‘œ í˜•ì‹ìœ¼ë¡œ ì¶œë ¥ë˜ë©°, ê° ê³µê³ ëŠ” ë²ˆí˜¸(idx), íšŒì‚¬ì´ë¦„, ê³µê³ ì´ë¦„, URLì„ í¬í•¨í•´ì•¼ í•´.
    URLì€ Streamlitì—ì„œ ì§€ì› ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë§í¬ë¥¼ ìƒì„±í•´ì•¼ í•˜ë©°, ë‹¤ìŒê³¼ ê°™ì´ ì¶œë ¥í•˜ë©´ ë¼:

    | idx | íšŒì‚¬ì´ë¦„ | ê³µê³ ì´ë¦„ | URL |
    |-----|----------|----------|-----|
    | 1   | íšŒì‚¬ì´ë¦„1 | ê³µê³ ì´ë¦„1 | [URL](í•´ë‹¹ url) |
    | 2   | íšŒì‚¬ì´ë¦„2 | ê³µê³ ì´ë¦„2 | [URL](í•´ë‹¹ url) |
    | ... | ...      | ...      | ... |

    Streamlitì—ì„œ ì´ í‘œë¥¼ ì¶œë ¥í•  ë•ŒëŠ” `st.markdown()` í•¨ìˆ˜ì™€ Markdown í…Œì´ë¸” í˜•ì‹ì„ í™œìš©í•˜ë©´ ë¼.

    # context: {context}
    # question: {question}
    # answer:
"""

    
prompt = ChatPromptTemplate.from_messages([
    ('system', system_message),
    MessagesPlaceholder(variable_name='chat_history'),
    ('human', '{question}'),
])

# ê²€ìƒ‰ ë° ì¶œë ¥ í•¨ìˆ˜
def search_jobs_with_llm(question):
    # ê²€ìƒ‰ëœ ê²°ê³¼ë¥¼ LLMì—ê²Œ ì „ë‹¬í•˜ê¸° ìœ„í•œ context ìƒì„±
    results = retriever.get_relevant_documents(question)
    context = "\n".join([result.page_content for result in results])

    # ë©”ëª¨ë¦¬ì—ì„œ ëŒ€í™” ê¸°ë¡ ë¡œë“œ
    memory_variables = st.session_state.memory.load_memory_variables({})
    chat_history = memory_variables.get('chat_history', [])

    # LLMì—ê²Œ ê²€ìƒ‰ëœ ê²°ê³¼ì™€ ì§ˆë¬¸ì„ ì „ë‹¬í•˜ì—¬ ê³µê³  ì¶œë ¥
    chain = (
        RunnablePassthrough.assign(context=RunnableLambda(lambda _: context))
        | RunnablePassthrough.assign(chat_history=RunnableLambda(lambda _: chat_history))
        | prompt
        | llm
    )
    
    # ì‘ë‹µ ìƒì„±
    response = chain.invoke({'question': question})
    
    # ì‘ë‹µì„ ë¬¸ìì—´ë¡œ ë³€í™˜ (content ì¶”ì¶œ)
    if hasattr(response, 'content'):
        response_text = response.content
    else:
        response_text = str(response)

    # ë©”ëª¨ë¦¬ì— ëŒ€í™” ê¸°ë¡ ì €ì¥
    st.session_state.memory.save_context(
        {'input': question},
        {'output': response_text},
    )

    return response_text

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state["messages"] = [{"role": "assistant", "content": "ì›í•˜ì‹œëŠ” ì§ë¬´, ì‹ ì…/ê²½ë ¥, ì§€ì—­ì„ ì„ íƒí•´ì£¼ì„¸ìš”"}]


# ì‚¬ì´ë“œë°”: ì§ë¬´, ê²½ë ¥, ì§€ì—­ ì„ íƒ ë° ì¡°íšŒ ë²„íŠ¼ ì¶”ê°€
with st.sidebar:
    st.title("ğŸ’¬ Job Search Chatbot")
    st.header("ê²€ìƒ‰ ì¡°ê±´")
    
    job_options = ['ë°ì´í„° ë¶„ì„ê°€', 'ë°ì´í„° ì—”ì§€ë‹ˆì–´', 'AI ê°œë°œì', 'ì±—ë´‡ ê°œë°œì',
                   'í´ë¼ìš°ë“œ ì—”ì§€ë‹ˆì–´', 'API ê°œë°œì', 'ë¨¸ì‹ ëŸ¬ë‹ ì—”ì§€ë‹ˆì–´', 'ë°ì´í„° ì‚¬ì´ì–¸í‹°ìŠ¤íŠ¸']
    selected_job = st.selectbox("ì§ë¬´ ì„ íƒ (ë³µìˆ˜ ì„ íƒì‚¬í•­):", job_options)
    
    exp_options = ['ì‹ ì…', 'ê²½ë ¥']
    selected_exp = st.selectbox("ê²½ë ¥ ì„ íƒ :", exp_options)
    
    loc_options = ['ì„œìš¸', 'ê²½ê¸°', 'ì¸ì²œ', 'ëŒ€ì „', 'ê´‘ì£¼', 'ëŒ€êµ¬', 'ìš¸ì‚°', 'ë¶€ì‚°', 'ê°•ì›',
                   'ì„¸ì¢…', 'ì¶©ë¶', 'ì¶©ë‚¨', 'ì „ë¶', 'ì „ë‚¨', 'ê²½ë¶', 'ê²½ë‚¨', 'ì œì£¼', 'í•´ì™¸']
    selected_loc = st.multiselect("ì§€ì—­ ì„ íƒ :", loc_options)
    
    # ì±„ìš© ê³µê³  ê²€ìƒ‰ ë²„íŠ¼
    if st.button("ğŸ” ì¡°íšŒ"):
        query = f"{selected_loc}ì—ì„œ {selected_exp}ì„ ì±„ìš©í•˜ëŠ” {selected_job} ê³µê³  ì•Œë ¤ì¤˜"
        st.session_state["messages"].append({
            "role": "user",
            "content": f"ì œê°€ ì„ íƒí•œ ì§ë¬´ëŠ” {selected_job}, ê²½ë ¥ì€ {selected_exp}, ì§€ì—­ì€ {selected_loc}ì…ë‹ˆë‹¤."
        })
        
        # LLMì— ê²€ìƒ‰ ìš”ì²­
        response = search_jobs_with_llm(query)
        st.session_state["messages"].append({
            "role": "assistant",
            "content": response
        })

# ì´ˆê¸° ì„¤ì •
st.caption("ğŸš€ A Streamlit chatbot powered by OpenAI")

# ê¸°ì¡´ ì±„íŒ… ë©”ì‹œì§€ ì¶œë ¥
for msg in st.session_state.messages:
    st.chat_message(msg["role"]).write(msg["content"])

# ë”ë³´ê¸° ë²„íŠ¼
if "displayed_results" in st.session_state and len(st.session_state["displayed_results"]) > 0:
    if st.button("ë” ë³´ê¸°"):
        query = f"{selected_loc}ì—ì„œ {selected_exp}ì„ ì±„ìš©í•˜ëŠ” {selected_job} ê³µê³  ì•Œë ¤ì¤˜"
        response = search_jobs_with_llm(query)
        st.session_state["messages"].append({
            "role": "assistant",
            "content": response
        })

# ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
if prompt := st.chat_input():
    st.session_state.messages.append({"role": "user", "content": prompt})
    st.chat_message("user").write(prompt)

    # OpenAI ì‘ë‹µ ìƒì„±
    response = search_jobs_with_llm(prompt)
    st.session_state.messages.append({"role": "assistant", "content": response})
    st.chat_message("assistant").write(response)
